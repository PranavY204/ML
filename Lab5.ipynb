{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Classification\n"
      ],
      "metadata": {
        "id": "yP_tuJnmfPDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9Vtuh2lYCyW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # Compute distances between x and all examples in the training set\n",
        "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        # Sort by distance and return indices of the first k neighbors\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        # Extract the labels of the k nearest neighbor training samples\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "        # Return the most common class label\n",
        "        most_common = np.bincount(k_nearest_labels).argmax()\n",
        "        return most_common"
      ],
      "metadata": {
        "id": "7_glmcJkYLJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('iris.csv')\n",
        "df['variety'].replace({'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfPPKn2XYhmB",
        "outputId": "3c73213c-048c-4cfd-e54a-579bbd153fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-22cf187c84df>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['variety'].replace({'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}, inplace=True)\n",
            "<ipython-input-3-22cf187c84df>:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['variety'].replace({'Setosa': 0, 'Versicolor': 1, 'Virginica': 2}, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1].values, df.iloc[:, -1].values, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_9AuCQLgaQf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNN()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "BoUzSjNna086"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = model.predict(X_test)\n",
        "actual_classes = y_test\n",
        "print(predicted_classes, actual_classes)\n",
        "correct = len([i for i in range(len(predicted_classes)) if predicted_classes[i] == actual_classes[i]])\n",
        "accuracy = correct / len(predicted_classes)\n",
        "print(\"Accuracy:\", accuracy*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfJOcq-2bZfK",
        "outputId": "64b2b9da-9580-4faa-f844-d6744a068971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0] [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Accuracy: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM Classification"
      ],
      "metadata": {
        "id": "1PazfN1efTKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Linear kernel function\n",
        "def linear_kernel(x1, x2):\n",
        "    return np.dot(x1, x2)\n",
        "\n",
        "# Support Vector Machine (SVM) class\n",
        "class SVM:\n",
        "    def __init__(self, C=1, max_iter=1000):\n",
        "        self.C = C  # Regularization parameter\n",
        "        self.max_iter = max_iter\n",
        "        self.alpha = None\n",
        "        self.b = None\n",
        "        self.kernel = linear_kernel\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    # Fit the SVM model (train the model)\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "        # One-vs-rest strategy: Train a separate SVM for each class\n",
        "        self.models = []\n",
        "        self.classes = np.unique(y)\n",
        "\n",
        "        for class_label in self.classes:\n",
        "            y_binary = np.where(y == class_label, 1, -1)  # Class vs all other classes\n",
        "            model = self._train_svm(X, y_binary)\n",
        "            self.models.append(model)\n",
        "\n",
        "    # Train a binary SVM for one class vs all others\n",
        "    def _train_svm(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        alpha = np.zeros(n_samples)\n",
        "        b = 0\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            alpha_prev = np.copy(alpha)\n",
        "            for i in range(n_samples):\n",
        "                j = np.random.choice([x for x in range(n_samples) if x != i])\n",
        "\n",
        "                E_i = self._compute_error(X, y, i, alpha, b)\n",
        "                E_j = self._compute_error(X, y, j, alpha, b)\n",
        "\n",
        "                alpha_i_old = alpha[i]\n",
        "                alpha_j_old = alpha[j]\n",
        "\n",
        "                L, H = self._compute_bounds(y, i, j, alpha)\n",
        "                if L == H:\n",
        "                    continue\n",
        "\n",
        "                eta = 2 * self.kernel(X[i], X[j]) - self.kernel(X[i], X[i]) - self.kernel(X[j], X[j])\n",
        "                if eta >= 0:\n",
        "                    continue\n",
        "\n",
        "                alpha[j] -= (y[j] * (E_i - E_j)) / eta\n",
        "                alpha[j] = min(H, max(L, alpha[j]))\n",
        "\n",
        "                if abs(alpha[j] - alpha_j_old) < 1e-5:\n",
        "                    continue\n",
        "\n",
        "                alpha[i] += y[i] * y[j] * (alpha_j_old - alpha[j])\n",
        "\n",
        "                b1 = b - E_i - y[i] * (alpha[i] - alpha_i_old) * self.kernel(X[i], X[i]) - y[j] * (alpha[j] - alpha_j_old) * self.kernel(X[i], X[j])\n",
        "                b2 = b - E_j - y[i] * (alpha[i] - alpha_i_old) * self.kernel(X[i], X[j]) - y[j] * (alpha[j] - alpha_j_old) * self.kernel(X[j], X[j])\n",
        "\n",
        "                if 0 < alpha[i] < self.C:\n",
        "                    b = b1\n",
        "                elif 0 < alpha[j] < self.C:\n",
        "                    b = b2\n",
        "                else:\n",
        "                    b = (b1 + b2) / 2\n",
        "\n",
        "            if np.linalg.norm(alpha - alpha_prev) < 1e-5:\n",
        "                break\n",
        "\n",
        "        return (alpha, b)\n",
        "\n",
        "    # Compute the error for a given sample\n",
        "    def _compute_error(self, X, y, i, alpha, b):\n",
        "        return np.dot(alpha * y, [self.kernel(X[i], X[j]) for j in range(len(X))]) + b - y[i]\n",
        "\n",
        "    # Compute the bounds L and H for the optimization problem\n",
        "    def _compute_bounds(self, y, i, j, alpha):\n",
        "        if y[i] != y[j]:\n",
        "            L = max(0, alpha[j] - alpha[i])\n",
        "            H = min(self.C, self.C + alpha[j] - alpha[i])\n",
        "        else:\n",
        "            L = max(0, alpha[i] + alpha[j] - self.C)\n",
        "            H = min(self.C, alpha[i] + alpha[j])\n",
        "        return L, H\n",
        "\n",
        "    # Predict the class labels for a given set of samples\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for i in range(len(X)):\n",
        "            class_scores = []\n",
        "            for model in self.models:\n",
        "                alpha, b = model\n",
        "                decision_value = 0\n",
        "                for j in range(len(self.X_train)):\n",
        "                    if alpha[j] > 0:\n",
        "                        decision_value += alpha[j] * self.y_train[j] * self.kernel(X[i], self.X_train[j])\n",
        "                decision_value += b\n",
        "                class_scores.append(decision_value)\n",
        "\n",
        "            # Choose the class with the highest decision value\n",
        "            predictions.append(self.classes[np.argmax(class_scores)])\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Preprocess the data: Standardize it\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM model\n",
        "svm = SVM(C=1, max_iter=1000)\n",
        "\n",
        "# Fit the SVM model\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = svm.predict(X_test)\n",
        "\n",
        "# Print the results\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Actual Labels:\", y_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqn7_bsrbi45",
        "outputId": "ba0332e8-173e-4b16-b472-8c2e59963201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0]\n",
            "Actual Labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Accuracy: 63.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm: SVM Classification (One-vs-Rest Strategy)\n",
        "\n",
        "1. Input:\n",
        "    - Training data: X_train (features), y_train (labels)\n",
        "    - Test data: X_test (features)\n",
        "    - Regularization parameter: C\n",
        "    - Maximum iterations: max_iter\n",
        "\n",
        "2. Initialization:\n",
        "    - Create an SVM object with C and max_iter.\n",
        "    - Identify unique class labels in y_train (self.classes).\n",
        "\n",
        "3. Training:\n",
        "    - For each class_label in self.classes:\n",
        "        a. Create binary labels (y_binary): 1 for current class, -1 for others.\n",
        "        b. Train a binary SVM model using _train_svm(X_train, y_binary).\n",
        "        c. Store the trained model (alpha, b) in self.models.\n",
        "\n",
        "4. Prediction:\n",
        "    - For each data point in X_test:\n",
        "        a. For each model in self.models:\n",
        "            i. Calculate the decision value using the kernel function and learned parameters (alpha, b).\n",
        "            ii. Store the decision value for the current class.\n",
        "        b. Predict the class label with the highest decision value.\n",
        "\n",
        "5. Output:\n",
        "    - Predicted labels for X_test.\n",
        "\n",
        "_train_svm(X, y):\n",
        "\n",
        "1. Initialization:\n",
        "    - alpha = 0 (for all training samples)\n",
        "    - b = 0\n",
        "\n",
        "2. Optimization loop (for max_iter iterations):\n",
        "    a. Store previous alpha values (alpha_prev).\n",
        "    b. For each training sample i:\n",
        "        i. Randomly select another sample j (different from i).\n",
        "        ii. Calculate errors E_i and E_j using _compute_error().\n",
        "        iii. Store old alpha values (alpha_i_old, alpha_j_old).\n",
        "        iv. Calculate bounds L and H using _compute_bounds().\n",
        "        v. If L == H, skip to the next iteration.\n",
        "        vi. Calculate learning rate eta.\n",
        "        vii. If eta >= 0, skip to the next iteration.\n",
        "        viii. Update alpha[j] using the update rule.\n",
        "        ix. Clip alpha[j] within bounds L and H.\n",
        "        x. If alpha[j] changed significantly, update alpha[i] and b.\n",
        "    c. If alpha values converged (change is small), break the loop.\n",
        "\n",
        "3. Output:\n",
        "    - Learned parameters (alpha, b).\n",
        "\n",
        "_compute_error(X, y, i, alpha, b):\n",
        "\n",
        "1. Calculate the error for sample i using the decision function and learned parameters.\n",
        "\n",
        "_compute_bounds(y, i, j, alpha):\n",
        "\n",
        "1. Calculate the lower (L) and upper (H) bounds for alpha[j] based on the labels of samples i and j and the regularization parameter C."
      ],
      "metadata": {
        "id": "DSbgsr5uqhqo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTmDQVrRmE2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}